{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fef7ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'objectif ici est de faire la fusion de nos deux précédents programmes. Celui de Jomuel et le mien.\n",
    "#A chaque fois on compare l'option chosie par els deux en partant de la structure du mien. \n",
    "#On a besoin d'être dans ce fichier afin d'accéder à notre dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d3bb01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. importations\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2af6140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_12220\\1833793343.py:2: DtypeWarning: Columns (36,38,46,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset=pd.read_csv(\"https://raw.githubusercontent.com/Jomuel-net/AI-finance-project/refs/heads/main/Data%20set/data_bank_alaska.csv\")\n"
     ]
    }
   ],
   "source": [
    "#a. Récupération des datasets\n",
    "dataset=pd.read_csv(\"https://raw.githubusercontent.com/Jomuel-net/AI-finance-project/refs/heads/main/Data%20set/data_bank_alaska.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90553c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28632 entries, 0 to 28631\n",
      "Data columns (total 54 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   agency_code                     28632 non-null  int64  \n",
      " 1   loan_amount_000s                28632 non-null  int64  \n",
      " 2   county_code                     28523 non-null  float64\n",
      " 3   census_tract_number             28521 non-null  float64\n",
      " 4   applicant_race_1                28632 non-null  int64  \n",
      " 5   applicant_race_2                551 non-null    float64\n",
      " 6   applicant_race_3                49 non-null     float64\n",
      " 7   applicant_race_4                24 non-null     float64\n",
      " 8   applicant_race_5                20 non-null     float64\n",
      " 9   co_applicant_race_1             28632 non-null  int64  \n",
      " 10  co_applicant_race_2             169 non-null    float64\n",
      " 11  co_applicant_race_3             17 non-null     float64\n",
      " 12  co_applicant_race_4             6 non-null      float64\n",
      " 13  co_applicant_race_5             5 non-null      float64\n",
      " 14  applicant_income_000s           25327 non-null  float64\n",
      " 15  edit_status_name                0 non-null      float64\n",
      " 16  population                      28521 non-null  float64\n",
      " 17  minority_population             28521 non-null  float64\n",
      " 18  hud_median_family_income        28521 non-null  float64\n",
      " 19  tract_to_msamd_income           28521 non-null  float64\n",
      " 20  number_of_owner_occupied_units  28521 non-null  float64\n",
      " 21  number_of_1_to_4_family_units   28521 non-null  float64\n",
      " 22  application_date_indicator      0 non-null      float64\n",
      " 23  state_name                      28632 non-null  object \n",
      " 24  agency_abbr                     28632 non-null  object \n",
      " 25  lien_status_name                28632 non-null  object \n",
      " 26  applicant_ethnicity_name        28632 non-null  object \n",
      " 27  agency_name                     28632 non-null  object \n",
      " 28  co_applicant_race_name_5        5 non-null      object \n",
      " 29  msamd_name                      22213 non-null  object \n",
      " 30  applicant_race_name_5           20 non-null     object \n",
      " 31  applicant_race_name_4           24 non-null     object \n",
      " 32  applicant_race_name_2           551 non-null    object \n",
      " 33  loan_type_name                  28632 non-null  object \n",
      " 34  co_applicant_sex_name           28632 non-null  object \n",
      " 35  denial_reason_name_2            503 non-null    object \n",
      " 36  co_applicant_ethnicity_name     28632 non-null  object \n",
      " 37  hoepa_status_name               28632 non-null  object \n",
      " 38  applicant_race_name_3           49 non-null     object \n",
      " 39  property_type_name              28632 non-null  object \n",
      " 40  owner_occupancy_name            28632 non-null  object \n",
      " 41  purchaser_type_name             28632 non-null  object \n",
      " 42  applicant_race_name_1           28632 non-null  object \n",
      " 43  co_applicant_race_name_3        17 non-null     object \n",
      " 44  co_applicant_race_name_4        6 non-null      object \n",
      " 45  co_applicant_race_name_2        169 non-null    object \n",
      " 46  denial_reason_name_1            2209 non-null   object \n",
      " 47  action_taken_name               28632 non-null  object \n",
      " 48  preapproval_name                28632 non-null  object \n",
      " 49  county_name                     28514 non-null  object \n",
      " 50  loan_purpose_name               28632 non-null  object \n",
      " 51  co_applicant_race_name_1        28632 non-null  object \n",
      " 52  denial_reason_name_3            50 non-null     object \n",
      " 53  applicant_sex_name              28632 non-null  object \n",
      "dtypes: float64(19), int64(4), object(31)\n",
      "memory usage: 11.8+ MB\n"
     ]
    }
   ],
   "source": [
    "#c. Feature Engineering\n",
    "#Rien d'implémenter de connu pour faire ce que l'on doit faire. \n",
    "#Le gros du travail qui va être fait: \n",
    "\n",
    "#Eléments à supprimer: \n",
    "\n",
    "#On va enlever les éléments qui ont des cases  vides dans: \n",
    "#C'est que ce sont des logements saisonniers/temporaires dans une zone de l'alaska sous sous peuplé.\n",
    "#Que 75000 habitants sur plusieurs millions. \n",
    "#Sinon on perd des données c'est dommage et si on les remplace par des 0 cela risque de mal influencer notre modèle. \n",
    "#Car on perd plein d'autres données. \n",
    "#On enlève donc : ceux qui n'ont pas de county_code \n",
    "#et ceux qui n'ont pas:   applicant_income_000s\n",
    "#On doit supprimer les éléments qui ont vide pour : applicant_income_000s et county_code \n",
    "#On crée L liste des éléments à supprimer:\n",
    "L=[]\n",
    "#pour county_code\n",
    "#on va créer un dataset composé uniquement de county_code\n",
    "#Basée sur la colonne \"county_code\": \n",
    "df_county = pd.DataFrame({\"dataset county code\":dataset[\"county_code\"]})\n",
    "#on va en récupérer toutes les lignes nulles\n",
    "l_nulles_county = df_county.isnull().values.any()\n",
    "#et les ajouter à L\n",
    "L+=l_nulles_county\n",
    "#pour applicant_income_000s\n",
    "#on va créer un dataset composé uniquement de applicant_income_000s\n",
    "#Basée sur la colonne \"applicant_income_000s\": \n",
    "df_income = pd.DataFrame({\"dataset applicant_income_000s\":dataset[\"applicant_income_000s\"]})\n",
    "#on va en récupérer toutes les lignes nulles\n",
    "l_nulles_income = df_income.isnull().values.any()\n",
    "#et les ajouter à L\n",
    "L+=l_nulles_income\n",
    "#suppression des lignes de notre liste:\n",
    "dataset = dataset.drop(index=L)\n",
    "\n",
    "\n",
    "#les colonnes dont nous devons nous occuper:\n",
    "\n",
    "#les colonnes à supprimer: on va en récupérer les noms \n",
    "#on va récupérer les inutiles et celles qui forment des doublons. Par exemple on a des colonnes dont on a du data \n",
    "#categorical et ensuite sa transcription en numerical. On garde alors juste la transcription en numerical\n",
    "#Les colonnes que l'on enlève et l'explication qui va avec: \n",
    "# Les inutiles\n",
    "#la première: as_of_year, respondent_id,  state_abbr, state_code, hoepa_status, edit_status, sequence_number,\n",
    "#application_date_indicator\n",
    "dataset= dataset.drop(columns=[\"as_of_year\", \"respondent_id\",  \"state_abbr\", \"state_code\", \"hoepa_status\", \"edit_status\", \"sequence_number\"])\n",
    "#et aussi: \n",
    "#denial_reason_1, denial_reason_2, denial_reason_3 et rate_spread\n",
    "#car beaucoup trop de données manquantes\n",
    "dataset = dataset.drop(columns=[\"denial_reason_1\", \"denial_reason_2\", \"denial_reason_3\", \"rate_spread\"])\n",
    "\n",
    "#Toutes celles où l'on a la transposition en numerical ensuite:\n",
    "#On garde toutes les colonnes qui sont en doublons avec les colonnes _name.\n",
    "#Car on va toutes les traiter avec OneHotEncoder plus simplement\n",
    "# On identifie toutes les colonnes qui finissent par \"_name\"\n",
    "name_cols = [col for col in dataset.columns if col.endswith(\"_name\")]\n",
    "# On identifie alors toutes les colonnes qui correspondent sont les transpositions\n",
    "#en numerical des colonnes qui finissent par name. \n",
    "radicals = [col[:-5] for col in name_cols]\n",
    "# On récupère alors les numéros de colonnes auquels correspondent ces colonnes\n",
    "cols_to_drop = [col for col in dataset.columns if col in radicals]\n",
    "#Et on enlève toutes ces colonnes\n",
    "dataset=dataset.drop(columns=cols_to_drop)\n",
    "#que nous n'ayons plus que des données numerical:\n",
    "#On applique OneHotEncoder quand il faut. \n",
    "\"\"\"pour la race de l'applicant notamment: \n",
    "On va utiliser oneHotencoder\n",
    "On ne va pas faire comme dans le dataset. On va créer 6 nouvelles features.\n",
    "Car on a quand même plus de 500 personnes qui ont plus d'une race et puis cela est toujours bon de savoir comment faire. \n",
    "Et à chaque fois pour chaque données on va mettre un 1 ou 0 pour savoir si la donnée a cette race ou non. \n",
    "n avait tout coder à la main avant de découvrir one hot encoder. \n",
    "On fait pareil pour la race du co applicant. Mais cette fois ci avec 8 catégories. \n",
    "On ne va pas faire comme dans le dataset. On va créer 8 nouvelles features.\n",
    "Car on a quand même plus de 150 personnes qui ont plus d'une race et puis cela est toujours bon de savoir comment faire. \n",
    "Et à chaque fois pour chaque données on va mettre un 1 ou 0 pour savoir si la donnée a cette race ou non. \n",
    "Et sinon bien sur on met un 1 dans notre provided. \"\"\"\n",
    "#On sélectionne les numerical features\n",
    "numerical_features=dataset.select_dtypes(include='number').columns.tolist()                         # stockage des données numériques (13 colonnes)\n",
    "#On en déduit les numerical features\n",
    "categorical_features=list(set(dataset.columns)-set(numerical_features))                             # stockage des données catégoriques(18 colonnes)\n",
    "#On va maintenant faire le preprocessing: \n",
    "#On définit la pipeline de preprocessing\n",
    "numeric= StandardScaler()\n",
    "categoric = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "preprocessor = ColumnTransformer(transformers=[(\"numeric\",numeric, numerical_features),\n",
    "(\"categoric\",categoric, categorical_features)])\n",
    "#et on l'applique sur les datas\n",
    "#data preparation des datas categorical\n",
    "X_categorical=dataset[categorical_features]\n",
    "#application\n",
    "X_categorical=preprocessor.fit_transform(X_categorical)\n",
    "#recréation du dataset total:\n",
    "dataclean= dataset[numerical_features]+X_categorical\n",
    "#On montre les datas\n",
    "dataclean.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
