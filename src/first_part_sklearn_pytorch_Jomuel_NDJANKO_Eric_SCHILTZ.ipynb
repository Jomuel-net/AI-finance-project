{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fdd90f",
   "metadata": {},
   "source": [
    "The aim of this predictive model is to predict if a certain person will receive a bank loan or not, using machine learning and deep learning tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fb375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importations sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, validation_curve, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585c2c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importations pytorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader                    # to build data set with pytorch\n",
    "import torch.nn as nn \n",
    "from torch.nn import Module                                                             # neural network\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0d272f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13708\\918415353.py:1: DtypeWarning: Columns (36,38,46,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset=pd.read_csv(\"https://raw.githubusercontent.com/Jomuel-net/AI-finance-project/refs/heads/main/Data%20set/data_bank_alaska.csv\")\n"
     ]
    }
   ],
   "source": [
    "dataset=pd.read_csv(\"https://raw.githubusercontent.com/Jomuel-net/AI-finance-project/refs/heads/main/Data%20set/data_bank_alaska.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be19f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28632 entries, 0 to 28631\n",
      "Data columns (total 78 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   as_of_year                      28632 non-null  int64  \n",
      " 1   respondent_id                   28632 non-null  object \n",
      " 2   agency_name                     28632 non-null  object \n",
      " 3   agency_abbr                     28632 non-null  object \n",
      " 4   agency_code                     28632 non-null  int64  \n",
      " 5   loan_type_name                  28632 non-null  object \n",
      " 6   loan_type                       28632 non-null  int64  \n",
      " 7   property_type_name              28632 non-null  object \n",
      " 8   property_type                   28632 non-null  int64  \n",
      " 9   loan_purpose_name               28632 non-null  object \n",
      " 10  loan_purpose                    28632 non-null  int64  \n",
      " 11  owner_occupancy_name            28632 non-null  object \n",
      " 12  owner_occupancy                 28632 non-null  int64  \n",
      " 13  loan_amount_000s                28632 non-null  int64  \n",
      " 14  preapproval_name                28632 non-null  object \n",
      " 15  preapproval                     28632 non-null  int64  \n",
      " 16  action_taken_name               28632 non-null  object \n",
      " 17  action_taken                    28632 non-null  int64  \n",
      " 18  msamd_name                      22213 non-null  object \n",
      " 19  msamd                           22213 non-null  float64\n",
      " 20  state_name                      28632 non-null  object \n",
      " 21  state_abbr                      28632 non-null  object \n",
      " 22  state_code                      28632 non-null  int64  \n",
      " 23  county_name                     28514 non-null  object \n",
      " 24  county_code                     28523 non-null  float64\n",
      " 25  census_tract_number             28521 non-null  float64\n",
      " 26  applicant_ethnicity_name        28632 non-null  object \n",
      " 27  applicant_ethnicity             28632 non-null  int64  \n",
      " 28  co_applicant_ethnicity_name     28632 non-null  object \n",
      " 29  co_applicant_ethnicity          28632 non-null  int64  \n",
      " 30  applicant_race_name_1           28632 non-null  object \n",
      " 31  applicant_race_1                28632 non-null  int64  \n",
      " 32  applicant_race_name_2           551 non-null    object \n",
      " 33  applicant_race_2                551 non-null    float64\n",
      " 34  applicant_race_name_3           49 non-null     object \n",
      " 35  applicant_race_3                49 non-null     float64\n",
      " 36  applicant_race_name_4           24 non-null     object \n",
      " 37  applicant_race_4                24 non-null     float64\n",
      " 38  applicant_race_name_5           20 non-null     object \n",
      " 39  applicant_race_5                20 non-null     float64\n",
      " 40  co_applicant_race_name_1        28632 non-null  object \n",
      " 41  co_applicant_race_1             28632 non-null  int64  \n",
      " 42  co_applicant_race_name_2        169 non-null    object \n",
      " 43  co_applicant_race_2             169 non-null    float64\n",
      " 44  co_applicant_race_name_3        17 non-null     object \n",
      " 45  co_applicant_race_3             17 non-null     float64\n",
      " 46  co_applicant_race_name_4        6 non-null      object \n",
      " 47  co_applicant_race_4             6 non-null      float64\n",
      " 48  co_applicant_race_name_5        5 non-null      object \n",
      " 49  co_applicant_race_5             5 non-null      float64\n",
      " 50  applicant_sex_name              28632 non-null  object \n",
      " 51  applicant_sex                   28632 non-null  int64  \n",
      " 52  co_applicant_sex_name           28632 non-null  object \n",
      " 53  co_applicant_sex                28632 non-null  int64  \n",
      " 54  applicant_income_000s           25327 non-null  float64\n",
      " 55  purchaser_type_name             28632 non-null  object \n",
      " 56  purchaser_type                  28632 non-null  int64  \n",
      " 57  denial_reason_name_1            2209 non-null   object \n",
      " 58  denial_reason_1                 2209 non-null   float64\n",
      " 59  denial_reason_name_2            503 non-null    object \n",
      " 60  denial_reason_2                 503 non-null    float64\n",
      " 61  denial_reason_name_3            50 non-null     object \n",
      " 62  denial_reason_3                 50 non-null     float64\n",
      " 63  rate_spread                     254 non-null    float64\n",
      " 64  hoepa_status_name               28632 non-null  object \n",
      " 65  hoepa_status                    28632 non-null  int64  \n",
      " 66  lien_status_name                28632 non-null  object \n",
      " 67  lien_status                     28632 non-null  int64  \n",
      " 68  edit_status_name                0 non-null      float64\n",
      " 69  edit_status                     0 non-null      float64\n",
      " 70  sequence_number                 0 non-null      float64\n",
      " 71  population                      28521 non-null  float64\n",
      " 72  minority_population             28521 non-null  float64\n",
      " 73  hud_median_family_income        28521 non-null  float64\n",
      " 74  tract_to_msamd_income           28521 non-null  float64\n",
      " 75  number_of_owner_occupied_units  28521 non-null  float64\n",
      " 76  number_of_1_to_4_family_units   28521 non-null  float64\n",
      " 77  application_date_indicator      0 non-null      float64\n",
      "dtypes: float64(26), int64(19), object(33)\n",
      "memory usage: 17.0+ MB\n",
      "   as_of_year respondent_id                            agency_name  \\\n",
      "0        2017    0000451965   Consumer Financial Protection Bureau   \n",
      "1        2017    0000005892   National Credit Union Administration   \n",
      "2        2017    0000451965   Consumer Financial Protection Bureau   \n",
      "3        2017    0000005912   National Credit Union Administration   \n",
      "4        2017    92-0164378  Federal Deposit Insurance Corporation   \n",
      "\n",
      "  agency_abbr  agency_code loan_type_name  loan_type  \\\n",
      "0        CFPB            9  VA-guaranteed          3   \n",
      "1        NCUA            5   Conventional          1   \n",
      "2        CFPB            9    FHA-insured          2   \n",
      "3        NCUA            5   Conventional          1   \n",
      "4        FDIC            3   Conventional          1   \n",
      "\n",
      "                                  property_type_name  property_type  \\\n",
      "0  One-to-four family dwelling (other than manufa...              1   \n",
      "1  One-to-four family dwelling (other than manufa...              1   \n",
      "2  One-to-four family dwelling (other than manufa...              1   \n",
      "3  One-to-four family dwelling (other than manufa...              1   \n",
      "4  One-to-four family dwelling (other than manufa...              1   \n",
      "\n",
      "  loan_purpose_name  ...  edit_status_name edit_status  sequence_number  \\\n",
      "0     Home purchase  ...               NaN         NaN              NaN   \n",
      "1     Home purchase  ...               NaN         NaN              NaN   \n",
      "2     Home purchase  ...               NaN         NaN              NaN   \n",
      "3     Home purchase  ...               NaN         NaN              NaN   \n",
      "4     Home purchase  ...               NaN         NaN              NaN   \n",
      "\n",
      "   population minority_population  hud_median_family_income  \\\n",
      "0      6702.0           17.170000                   89400.0   \n",
      "1         NaN                 NaN                       NaN   \n",
      "2     11450.0           20.030001                   84800.0   \n",
      "3      3864.0           22.490000                   89400.0   \n",
      "4      1941.0           56.880001                   89400.0   \n",
      "\n",
      "  tract_to_msamd_income  number_of_owner_occupied_units  \\\n",
      "0             94.010002                          1651.0   \n",
      "1                   NaN                             NaN   \n",
      "2             99.690002                          2923.0   \n",
      "3             71.989998                           979.0   \n",
      "4             58.340000                           179.0   \n",
      "\n",
      "  number_of_1_to_4_family_units  application_date_indicator  \n",
      "0                        2219.0                         NaN  \n",
      "1                           NaN                         NaN  \n",
      "2                        3992.0                         NaN  \n",
      "3                        1328.0                         NaN  \n",
      "4                         318.0                         NaN  \n",
      "\n",
      "[5 rows x 78 columns]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "head=dataset.head()\n",
    "info=dataset.info()\n",
    "print(head)\n",
    "print(info)                                                                         # 28632 rows and 78 columns, there are 3 Dtypes ( float, int and object=[string] )\n",
    "target_name= \"action_taken_name\"                                                    # purpose name\n",
    "data , target = dataset.drop(columns=target_name), dataset[target_name]             # data to browse and target\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9351e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\USER\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "#c. Feature Engineering\n",
    "#Rien d'implémenter de connu pour faire ce que l'on doit faire. \n",
    "#Le gros du travail qui va être fait: \n",
    "\n",
    "#Lien nécessaire entre eric et jomuel\n",
    "dataset = data\n",
    "\n",
    "#Eléments à supprimer: \n",
    "\n",
    "#On va enlever les éléments qui ont des cases  vides dans: \n",
    "#C'est que ce sont des logements saisonniers/temporaires dans une zone de l'alaska sous sous peuplé.\n",
    "#Que 75000 habitants sur plusieurs millions. \n",
    "#Sinon on perd des données c'est dommage et si on les remplace par des 0 cela risque de mal influencer notre modèle. \n",
    "#Car on perd plein d'autres données. \n",
    "#On enlève donc : ceux qui n'ont pas de county_code \n",
    "#et ceux qui n'ont pas:   applicant_income_000s\n",
    "#On doit supprimer les éléments qui ont vide pour : applicant_income_000s et county_code \n",
    "#On crée L liste des éléments à supprimer:\n",
    "L=[]\n",
    "#pour county_code\n",
    "#on va créer un dataset composé uniquement de county_code\n",
    "#Basée sur la colonne \"county_code\": \n",
    "df_county = pd.DataFrame({\"dataset county code\":dataset[\"county_code\"]})\n",
    "#on va en récupérer toutes les lignes nulles\n",
    "l_nulles_county = df_county.isnull().values.any()\n",
    "#et les ajouter à L\n",
    "L+=l_nulles_county\n",
    "#pour applicant_income_000s\n",
    "#on va créer un dataset composé uniquement de applicant_income_000s\n",
    "#Basée sur la colonne \"applicant_income_000s\": \n",
    "df_income = pd.DataFrame({\"dataset applicant_income_000s\":dataset[\"applicant_income_000s\"]})\n",
    "#on va en récupérer toutes les lignes nulles\n",
    "l_nulles_income = df_income.isnull().values.any()\n",
    "#et les ajouter à L\n",
    "L+=l_nulles_income\n",
    "#suppression des lignes de notre liste:\n",
    "dataset = dataset.drop(index=L)\n",
    "\n",
    "\n",
    "#les colonnes dont nous devons nous occuper:\n",
    "\n",
    "#les colonnes à supprimer: on va en récupérer les noms \n",
    "#on va récupérer les inutiles et celles qui forment des doublons. Par exemple on a des colonnes dont on a du data \n",
    "#categorical et ensuite sa transcription en numerical. On garde alors juste la transcription en numerical\n",
    "#Les colonnes que l'on enlève et l'explication qui va avec: \n",
    "# Les inutiles\n",
    "#la première: as_of_year, respondent_id,  state_abbr, state_code, hoepa_status, edit_status, sequence_number,\n",
    "#application_date_indicator\n",
    "dataset= dataset.drop(columns=[\"as_of_year\", \"respondent_id\",  \"state_abbr\", \"state_code\", \"hoepa_status\", \"edit_status\", \"sequence_number\"])\n",
    "#et aussi: \n",
    "#denial_reason_1, denial_reason_2, denial_reason_3 et rate_spread\n",
    "#car beaucoup trop de données manquantes\n",
    "dataset = dataset.drop(columns=[\"denial_reason_1\", \"denial_reason_2\", \"denial_reason_3\", \"rate_spread\"])\n",
    "\n",
    "#Toutes celles où l'on a la transposition en numerical ensuite:\n",
    "#On garde toutes les colonnes qui sont en doublons avec les colonnes _name.\n",
    "#Car on va toutes les traiter avec OneHotEncoder plus simplement\n",
    "# On identifie toutes les colonnes qui finissent par \"_name\"\n",
    "name_cols = [col for col in dataset.columns if col.endswith(\"_name\")]\n",
    "# On identifie alors toutes les colonnes qui correspondent sont les transpositions\n",
    "#en numerical des colonnes qui finissent par name. \n",
    "radicals = [col[:-5] for col in name_cols]\n",
    "# On récupère alors les numéros de colonnes auquels correspondent ces colonnes\n",
    "cols_to_drop = [col for col in dataset.columns if col in radicals]\n",
    "#Et on enlève toutes ces colonnes\n",
    "dataset=dataset.drop(columns=cols_to_drop)\n",
    "#que nous n'ayons plus que des données numerical:\n",
    "#On applique OneHotEncoder quand il faut. \n",
    "\"\"\"pour la race de l'applicant notamment: \n",
    "On va utiliser oneHotencoder\n",
    "On ne va pas faire comme dans le dataset. On va créer 6 nouvelles features.\n",
    "Car on a quand même plus de 500 personnes qui ont plus d'une race et puis cela est toujours bon de savoir comment faire. \n",
    "Et à chaque fois pour chaque données on va mettre un 1 ou 0 pour savoir si la donnée a cette race ou non. \n",
    "n avait tout coder à la main avant de découvrir one hot encoder. \n",
    "On fait pareil pour la race du co applicant. Mais cette fois ci avec 8 catégories. \n",
    "On ne va pas faire comme dans le dataset. On va créer 8 nouvelles features.\n",
    "Car on a quand même plus de 150 personnes qui ont plus d'une race et puis cela est toujours bon de savoir comment faire. \n",
    "Et à chaque fois pour chaque données on va mettre un 1 ou 0 pour savoir si la donnée a cette race ou non. \n",
    "Et sinon bien sur on met un 1 dans notre provided. \"\"\"\n",
    "#On sélectionne les numerical features\n",
    "numerical_features=dataset.select_dtypes(include='number').columns.tolist()                         # stockage des données numériques (13 colonnes)\n",
    "#On en déduit les categorical features\n",
    "categorical_features=list(set(dataset.columns)-set(numerical_features))                             # stockage des données catégoriques(18 colonnes)\n",
    "#On va maintenant faire le preprocessing: \n",
    "#On définit la pipeline de preprocessing\n",
    "numeric= StandardScaler()\n",
    "categoric = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "preprocessor = ColumnTransformer(transformers=[(\"numeric\",numeric, numerical_features),\n",
    "(\"categoric\",categoric, categorical_features)])\n",
    "#et on l'applique sur les datas\n",
    "#data preparation\n",
    "X_full=dataset[numerical_features+categorical_features]\n",
    "#data_clean = pd.DataFrame({\"dataset final\": X_full})\n",
    "#application\n",
    "dataset=preprocessor.fit_transform(X_full)\n",
    "X_full = dataset\n",
    "#On renomme X_full qui n'est plus un dataset on le rappelle. \n",
    "data_clean=X_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ff5ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_full  (10-fold CV) : 0.98\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model pipeloine and scaling complete\n",
    "model_pipeline_full=Pipeline([ \n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# data preparation\n",
    "Y= target\n",
    "\n",
    "# training\n",
    "model_pipeline_full.fit(X_full,Y)\n",
    "\n",
    "# predictive model\n",
    "y_pred=model_pipeline_full.predict(X_full)\n",
    "\n",
    "# evaluation of the model using cross validation\n",
    "\n",
    "cv_results= cross_validate(\n",
    "    model_pipeline_full,\n",
    "    X_full,\n",
    "    Y,\n",
    "    cv=StratifiedKFold(n_splits=10),\n",
    "    scoring= 'accuracy',\n",
    "    return_train_score= True,\n",
    "    return_estimator= True\n",
    ")\n",
    "# print the accuracy\n",
    "\n",
    "mean_accuracy_full = cv_results['test_score'].mean()\n",
    "print(f\"Accuracy_full  (10-fold CV) : {mean_accuracy_full:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b9bd3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_taken_name\n",
      "Loan originated                                14430\n",
      "Loan purchased by the institution               6415\n",
      "Application withdrawn by applicant              3495\n",
      "Application denied by financial institution     3075\n",
      "File closed for incompleteness                   756\n",
      "Application approved but not accepted            461\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(Y).value_counts())   # occurrences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c9da2",
   "metadata": {},
   "source": [
    "Let's Build and train a classification MLP on our data  with pytorch tools.\n",
    "\n",
    "Step 1: Load the dataset using sklearn and create a custom PyTorch Dataset for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd57d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# implementation of the MLP\n",
    "\n",
    "encoder=LabelEncoder()                                                                  # instance encoder\n",
    "target_encoded=encoder.fit_transform(target)                                            # target encoding for tensor management\n",
    "\n",
    "#X_prepared=preprocessor.fit_transform(X_full)                                           # prepares X_full for pytorch( in numpy)\n",
    "\n",
    "X=torch.tensor(X_full,dtype=torch.float32)                                          # data tensor building\n",
    "\n",
    "means = X.mean(dim=0, keepdim=True)                                                     # mean\n",
    "stds = X.std(dim=0, keepdim=True)                                                       # standard deviation(sigma)\n",
    " \n",
    "stds[stds == 0] = 1.0                                                                   # avoid the 0-division \n",
    "\n",
    "X_standardized= (X - means) / stds                                                      # scaling\n",
    "\n",
    "y=torch.tensor(target_encoded, dtype=torch.long)                                        # target tensor building                                           \n",
    "\n",
    "data_set_torch=TensorDataset(X_standardized,y)                                          # data set building \n",
    "\n",
    "sample0, target0 = data_set_torch[0]\n",
    "print (sample0.shape, target0.shape)                                                    # on obtient des vecteurs de dimensiosn 328 et le tzrget etant scalaore "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9062b3",
   "metadata": {},
   "source": [
    "\n",
    "Step 2: Create data loaders for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67009deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# define sizes for each step of the learning\n",
    "train_size=len(data_set_torch)*80//100\n",
    "valid_size=len(data_set_torch)*20//100\n",
    "test_size=len(data_set_torch) - (train_size + valid_size)\n",
    "train_dataset, valid_dataset , test_dataset = random_split(           \n",
    "                                data_set_torch,\n",
    "                                [train_size,valid_size,test_size]\n",
    "                                ) \n",
    "# data loaders (to load data)\n",
    "batch_size=32                                       # because it's a cpu\n",
    "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "valid_loader=DataLoader(valid_dataset,batch_size=batch_size)\n",
    "test_loader=DataLoader(test_dataset,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82ed6d",
   "metadata": {},
   "source": [
    "Step 3: Let's build a custom MLP module to tackle our classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e33cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by the sequential way (most obvious)\n",
    "X_tensor, y_tensor=data_set_torch.tensors\n",
    "n_inputs= X_tensor.shape[1]\n",
    "n_classes= len(torch.unique(y_tensor))                      # number of differents categories to predict  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set the model\n",
    "bank_loan_model = nn.Sequential(                       \n",
    "    nn.Linear(n_inputs,200), nn.ReLU(),                    # linear layer , ReLU activation      \n",
    "    nn.Linear(200,100), nn.ReLU(),\n",
    "    nn.Linear(100,50), nn.ReLU(),\n",
    "    nn.Linear(50, n_classes)\n",
    "    ).to(device)                                           # puts on the cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6828e2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan],\n",
       " 'train_metric': [0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171,\n",
       "  0.01685221493244171],\n",
       " 'valid_metric': [0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328,\n",
       "  0.013098148629069328]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "     # let's build our training\n",
    "n_epochs = 30\n",
    "learning_rate= [0.01, 0.001 , 0.0001]\n",
    "\n",
    "def train(\n",
    "        \n",
    "        model:Module,\n",
    "        optimizer: Optimizer,\n",
    "        criterion: _Loss ,                                                                            # criteria\n",
    "        metric: Metric,                                                                               # metric\n",
    "        train_loader: DataLoader , \n",
    "        valid_loader: DataLoader ,\n",
    "        n_epochs:int,\n",
    "        device: str = \"cpu\"\n",
    "       \n",
    "        )  :\n",
    "    \n",
    "        history={ \"train_loss\":[] , \"train_metric\":[], \"valid_metric\": []    }                             # initilization of the output\n",
    "\n",
    "    \n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            model.train()                                                                                  # sets the model in a training mode     \n",
    "            train_loss=0.0                                                                                 # initialization\n",
    "            metric.reset()\n",
    "\n",
    "            for X_batch , y_batch in train_loader :\n",
    "                X_batch, y_batch = X_batch.to(device),y_batch.to(device)                                   # sets on the device \n",
    "                y_pred=model(X_batch)\n",
    "                loss = criterion(y_pred,y_batch)\n",
    "                loss.backward()                                                                            # back propagation\n",
    "                optimizer.step()                                                                           # updates model parameters\n",
    "                optimizer.zero_grad()                                                                      # renitialize gradient to zero to avoid accumulation\n",
    "                train_loss+= loss.item()       \n",
    "                metric.update(y_pred,y_batch)                                                              # performance metric       \n",
    "\n",
    "            train_loss/=len(train_loader)\n",
    "            train_metric=metric.compute().item()\n",
    "\n",
    "            # validation step\n",
    "\n",
    "            model.eval()                                                                                     # set the model in a validation mode\n",
    "            metric.reset()\n",
    "            with torch.no_grad():                                                                            # avoid to compute gradient\n",
    "                for X_val, y_val in valid_loader:\n",
    "                    X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                    outputs = model(X_val)\n",
    "                    metric.update(outputs, y_val)\n",
    "\n",
    "            valid_metric = metric.compute().item()\n",
    "\n",
    "\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"train_metric\"].append(train_metric)\n",
    "            history[\"valid_metric\"].append(valid_metric)\n",
    "\n",
    "        return history\n",
    "\n",
    "\n",
    "\n",
    "# let's train our model\n",
    "for lr in learning_rate:\n",
    "\n",
    "    optimizer= torch.optim.SGD(bank_loan_model.parameters(), lr=lr)                            # optimizer with stochastic gradient descent\n",
    "    criterion= nn.CrossEntropyLoss ()       \n",
    "    metric= torchmetrics.Accuracy(task=\"multiclass\",num_classes=n_classes).to(device)    \n",
    "\n",
    "# out put\n",
    "\n",
    "train(bank_loan_model, optimizer, criterion, metric, train_loader,                          # return the output of the training\n",
    "                    valid_loader, n_epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3d63e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test step with test_loader\n",
    "\n",
    "def evaluate(\n",
    "    model: Module,\n",
    "    criterion: _Loss,\n",
    "    test_loader: DataLoader,\n",
    "    device: str = \"cpu\",\n",
    "    aggregate_fn=torch.mean\n",
    "\n",
    "    ):\n",
    "\n",
    "    model.eval()          \n",
    "    metrics=[]    \n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            metrics.append(loss)\n",
    "            \n",
    "\n",
    "            \n",
    "    return aggregate_fn(torch.stack(metrics))\n",
    "\n",
    "\n",
    "evaluate(bank_loan_model,criterion=nn.CrossEntropyLoss(), test_loader=test_loader )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
