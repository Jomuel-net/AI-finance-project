{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fb375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa0d272f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_11536\\1282787046.py:2: DtypeWarning: Columns (36,38,46,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset=pd.read_csv(\"https://raw.githubusercontent.com/Jomuel-net/AI-finance-project/refs/heads/main/Data%20set/data_bank_alaska.csv\")\n"
     ]
    }
   ],
   "source": [
    "#importation du dataset\n",
    "dataset=pd.read_csv(\"https://raw.githubusercontent.com/Jomuel-net/AI-finance-project/refs/heads/main/Data%20set/data_bank_alaska.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be19f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notre objectif va être de se baser sur ce dataset et de créer un modèle.\n",
    "#On va faire un module par partie nécessaire de notre modèle. et on save indépendamment du github de notre côté afin\n",
    "#d'être sur à la fin de notre travail aujourd'hui. On télécharge data_exploration à l'aide de file puis Save AS..\n",
    "#on va coder tout notre modèle ici afin de simplifier pour l'instant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35484757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28632 entries, 0 to 28631\n",
      "Data columns (total 78 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   as_of_year                      28632 non-null  int64  \n",
      " 1   respondent_id                   28632 non-null  object \n",
      " 2   agency_name                     28632 non-null  object \n",
      " 3   agency_abbr                     28632 non-null  object \n",
      " 4   agency_code                     28632 non-null  int64  \n",
      " 5   loan_type_name                  28632 non-null  object \n",
      " 6   loan_type                       28632 non-null  int64  \n",
      " 7   property_type_name              28632 non-null  object \n",
      " 8   property_type                   28632 non-null  int64  \n",
      " 9   loan_purpose_name               28632 non-null  object \n",
      " 10  loan_purpose                    28632 non-null  int64  \n",
      " 11  owner_occupancy_name            28632 non-null  object \n",
      " 12  owner_occupancy                 28632 non-null  int64  \n",
      " 13  loan_amount_000s                28632 non-null  int64  \n",
      " 14  preapproval_name                28632 non-null  object \n",
      " 15  preapproval                     28632 non-null  int64  \n",
      " 16  action_taken_name               28632 non-null  object \n",
      " 17  action_taken                    28632 non-null  int64  \n",
      " 18  msamd_name                      22213 non-null  object \n",
      " 19  msamd                           22213 non-null  float64\n",
      " 20  state_name                      28632 non-null  object \n",
      " 21  state_abbr                      28632 non-null  object \n",
      " 22  state_code                      28632 non-null  int64  \n",
      " 23  county_name                     28514 non-null  object \n",
      " 24  county_code                     28523 non-null  float64\n",
      " 25  census_tract_number             28521 non-null  float64\n",
      " 26  applicant_ethnicity_name        28632 non-null  object \n",
      " 27  applicant_ethnicity             28632 non-null  int64  \n",
      " 28  co_applicant_ethnicity_name     28632 non-null  object \n",
      " 29  co_applicant_ethnicity          28632 non-null  int64  \n",
      " 30  applicant_race_name_1           28632 non-null  object \n",
      " 31  applicant_race_1                28632 non-null  int64  \n",
      " 32  applicant_race_name_2           551 non-null    object \n",
      " 33  applicant_race_2                551 non-null    float64\n",
      " 34  applicant_race_name_3           49 non-null     object \n",
      " 35  applicant_race_3                49 non-null     float64\n",
      " 36  applicant_race_name_4           24 non-null     object \n",
      " 37  applicant_race_4                24 non-null     float64\n",
      " 38  applicant_race_name_5           20 non-null     object \n",
      " 39  applicant_race_5                20 non-null     float64\n",
      " 40  co_applicant_race_name_1        28632 non-null  object \n",
      " 41  co_applicant_race_1             28632 non-null  int64  \n",
      " 42  co_applicant_race_name_2        169 non-null    object \n",
      " 43  co_applicant_race_2             169 non-null    float64\n",
      " 44  co_applicant_race_name_3        17 non-null     object \n",
      " 45  co_applicant_race_3             17 non-null     float64\n",
      " 46  co_applicant_race_name_4        6 non-null      object \n",
      " 47  co_applicant_race_4             6 non-null      float64\n",
      " 48  co_applicant_race_name_5        5 non-null      object \n",
      " 49  co_applicant_race_5             5 non-null      float64\n",
      " 50  applicant_sex_name              28632 non-null  object \n",
      " 51  applicant_sex                   28632 non-null  int64  \n",
      " 52  co_applicant_sex_name           28632 non-null  object \n",
      " 53  co_applicant_sex                28632 non-null  int64  \n",
      " 54  applicant_income_000s           25327 non-null  float64\n",
      " 55  purchaser_type_name             28632 non-null  object \n",
      " 56  purchaser_type                  28632 non-null  int64  \n",
      " 57  denial_reason_name_1            2209 non-null   object \n",
      " 58  denial_reason_1                 2209 non-null   float64\n",
      " 59  denial_reason_name_2            503 non-null    object \n",
      " 60  denial_reason_2                 503 non-null    float64\n",
      " 61  denial_reason_name_3            50 non-null     object \n",
      " 62  denial_reason_3                 50 non-null     float64\n",
      " 63  rate_spread                     254 non-null    float64\n",
      " 64  hoepa_status_name               28632 non-null  object \n",
      " 65  hoepa_status                    28632 non-null  int64  \n",
      " 66  lien_status_name                28632 non-null  object \n",
      " 67  lien_status                     28632 non-null  int64  \n",
      " 68  edit_status_name                0 non-null      float64\n",
      " 69  edit_status                     0 non-null      float64\n",
      " 70  sequence_number                 0 non-null      float64\n",
      " 71  population                      28521 non-null  float64\n",
      " 72  minority_population             28521 non-null  float64\n",
      " 73  hud_median_family_income        28521 non-null  float64\n",
      " 74  tract_to_msamd_income           28521 non-null  float64\n",
      " 75  number_of_owner_occupied_units  28521 non-null  float64\n",
      " 76  number_of_1_to_4_family_units   28521 non-null  float64\n",
      " 77  application_date_indicator      0 non-null      float64\n",
      "dtypes: float64(26), int64(19), object(33)\n",
      "memory usage: 17.0+ MB\n"
     ]
    }
   ],
   "source": [
    "#a. Récupération des datasets\n",
    "#on a récupérer au dessus notre dataset\n",
    "#test de bonne récupération. Et que l'on pourra facilement le gérer par la suite:\n",
    "#un brin de feature engineering pour cela:\n",
    "dataset.info()\n",
    "#On a bien notre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "717d34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. Importations \n",
    "#Que l'on remplira au fur et à mesure.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, validation_curve, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b05b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_11536\\1692075978.py:50: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dataset[[\"msamd\", \"county_code\", \"census_tract_number\"]] = dataset[[\"msamd\", \"county_code\", \"census_tract_number\"]].applymap(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28632\n",
      "28632\n"
     ]
    }
   ],
   "source": [
    "#c. Feature Engineering\n",
    "#Rien d'implémenter de connu pour faire ce que l'on doit faire. \n",
    "#Le gros du travail qui va être fait: \n",
    "#On va partir du principe faut que l'on s'assure:\n",
    "#les colonnes dont nous devons nous occuper:\n",
    "\n",
    "#Eléments à supprimer: \n",
    "#On va enlever les éléments qui ont des cases  vides dans: \n",
    "#C'est que ce sont des logements saisonniers/temporaires dans une zone de l'alaska sous sous peuplé.\n",
    "#Que 75000     habitants sur plusieurs millions. \n",
    "#Sinon on perd des données c'est dommage et si on les remplace par des 0 cela risque de mal influencer notre modèle. \n",
    "#ceux qui n'ont pas de county_code \n",
    "#et pour le:\n",
    "#applicant_income_000s\n",
    "#On doit supprimer les éléments qui ont vide pour cette valeur. \n",
    "#Peut être le mettre plus haut pour simplifier les boucles for précédentes. \n",
    "n=len(dataset)\n",
    "print(n)\n",
    "\n",
    "\n",
    "#les colonnes à supprimer: on va en récupérer les noms \n",
    "#on va récupérer les inutiles et celles qui forment des doublons. Par exemple on a des colonnes dont on a du data \n",
    "#categorical et ensuite sa transcription en numerical. On garde alors juste la transcription en numerical\n",
    "#Les colonnes que l'on enlève et l'explication qui va avec: \n",
    "# Les inutiles\n",
    "#la première: as_of_year, respondent_id,  state_abbr, state_code, hoepa_status, edit_status, sequence_number,\n",
    "#application_date_indicator\n",
    "dataset= dataset.drop(columns=[\"as_of_year\", \"respondent_id\",  \"state_abbr\", \"state_code\", \"hoepa_status\", \"edit_status\", \"sequence_number\"])\n",
    "\n",
    "#Toutes celles où l'on a la transposition en numerical ensuite:\n",
    "#agency_name, agency_abbr, loan_type_name, property_ype_name, loan_purpose_name, owner_occupancy_name,\n",
    "#preapproval_name, action_taken_name, msamd_name, county_name, applicant_ethnicity_name, co_applicant_ethnicity_name,\n",
    "#applicant_race_name_1, applicant_race_name_2, applicant_race_name_3, applicant_race_name_4, applicant_race_name_5\n",
    "#co_applicant_race_name_1, co_applicant_race_name_2, co_applicant_race_name_3, co_applicant_race_name_4,\n",
    "#co_applicant_race_name_5, applicant_sex_name, co_applicant_sex_name, purchaser_type_name, \n",
    "#denial_reason_name_1, denial_reason_name_2, denial_reason_name_3, hoepa_status_name, lien_status_name\n",
    "#edit_status_name, state_name,\n",
    "#toutes les colonnes où il y a name en fait. \n",
    "dataset= dataset.drop(columns=[\"agency_name\", \"agency_abbr\", \"loan_type_name\", \"property_type_name\", \"loan_purpose_name\", \"owner_occupancy_name\",\n",
    "    \"preapproval_name\", \"action_taken_name\", \"msamd_name\", \"county_name\", \"applicant_ethnicity_name\", \"co_applicant_ethnicity_name\",\n",
    "    \"applicant_race_name_1\", \"applicant_race_name_2\", \"applicant_race_name_3\", \"applicant_race_name_4\", \"applicant_race_name_5\",\n",
    "    \"co_applicant_race_name_1\", \"co_applicant_race_name_2\", \"co_applicant_race_name_3\", \"co_applicant_race_name_4\",\n",
    "    \"co_applicant_race_name_5\", \"applicant_sex_name\", \"co_applicant_sex_name\", \"purchaser_type_name\", \n",
    "    \"denial_reason_name_1\", \"denial_reason_name_2\", \"denial_reason_name_3\", \"hoepa_status_name\", \"lien_status_name\",\n",
    "    \"edit_status_name\", \"state_name\"])\n",
    "\n",
    "\n",
    "#que toutes les colonnes soient remplies. Quitte à en supprimer certaines ou en créer de nouvelles.\n",
    "\n",
    "#A mettre des 1 dans les vides cases vides pour:    \n",
    "# msamd, county_code, census_tract_number\n",
    "#Pourquoi : \n",
    "#pour msamd les colonnes non remplis c'est que le logement ne se trouve pas dans une métropole de l'Alaska\n",
    "#On peut alors convenir d'un nombre pour ce fait de ne pas être dans une métropole. \n",
    "#pour les county de même on a tout une partie de l'alaska qui n'appartient à aucun coutny et est dirigé directement\n",
    "#par l'état. \n",
    "#Attention toujours deux valeurs vides dans notre tableau sur excel. A faire gaffe si pareil sur csv. \n",
    "#On va créer la fonction qui va modifier nos colonnes. Valeur par valeur\n",
    "def f(a):\n",
    "    if a==None: \n",
    "            #on met 1 à place\n",
    "            a=1\n",
    "#et ensuite modifier chaque colonne avec \n",
    "dataset[[\"msamd\", \"county_code\", \"census_tract_number\"]] = dataset[[\"msamd\", \"county_code\", \"census_tract_number\"]].applymap(f)\n",
    "\n",
    "#pour la race de l'applicant. \n",
    "# On va utiliser oneHotencoder\n",
    "# On ne va pas faire comme dans le dataset. On va créer 6 nouvelles features.\n",
    "#Car on a quand même plus de 500 personnes qui ont plus d'une race et puis cela est toujours bon de savoir comment faire. \n",
    "#Et à chaque fois pour chaque données on va mettre un 1 ou 0 pour savoir si la donnée a cette race ou non. \n",
    "\n",
    "\"\"\"#Et sinon bien sur on met un 1 dans notre provided. \n",
    "n=len(dataset)\n",
    "print(n)\n",
    "#On va créer nos 6 nouvelles colonnes:\n",
    "#de longueur n et de noms: race1, race2, race3, race4, race5, race6\n",
    "b1=pd.DataFrame({\"race1\": [0 for i in range(n)]})\n",
    "b2=pd.DataFrame({\"race2\": [0 for i in range(n)]})\n",
    "b3=pd.DataFrame({\"race3\": [0 for i in range(n)]})\n",
    "b4=pd.DataFrame({\"race4\": [0 for i in range(n)]})\n",
    "b5=pd.DataFrame({\"race5\": [0 for i in range(n)]})\n",
    "b6=pd.DataFrame({\"race6\": [0 for i in range(n)]})\n",
    "#On va parcourir les éléments de notre dataset. de taille n. \n",
    "for k in range(n):\n",
    "    #Pour chaque élément: On va regarder leurs valeurs dans le colonnes de races\n",
    "    #On a 5 colonnes de race. \n",
    "    valrace1=dataset.loc[k, \"applicant_race_1\"]\n",
    "    valrace2=dataset.loc[k, \"applicant_race_2\"]\n",
    "    valrace3=dataset.loc[k, \"applicant_race_3\"]\n",
    "    valrace4=dataset.loc[k, \"applicant_race_4\"]\n",
    "    valrace5=dataset.loc[k, \"applicant_race_5\"]\n",
    "    #et le stocker dans une liste\n",
    "    L=[valrace1,valrace2,valrace3,valrace4,valrace5]\n",
    "    #On va alors parcourir cette liste et en fonction de ce qu'elle contient on va remplir \n",
    "    #ou non les 6 colonnes crées avant\n",
    "    #avec 1 si oui l'élément a cette race et on fait rien sinon sinon. \n",
    "    #pour race1: \n",
    "    if 1 in L:\n",
    "         b1.loc[k,\"race1\"]=1\n",
    "    #pour race2: \n",
    "    if 2 in L:\n",
    "         b2.loc[k,\"race2\"]=1\n",
    "    #pour race1: \n",
    "    if 3 in L:\n",
    "         b3.loc[k,\"race3\"]=1\n",
    "    #pour race1: \n",
    "    if 4 in L:\n",
    "         b4.loc[k,\"race4\"]=1\n",
    "    #pour race1: \n",
    "    if 5 in L:\n",
    "         b5.loc[k,\"race5\"]=1\n",
    "    #pour race1: \n",
    "    if 6 in L:\n",
    "         b6.loc[k,\"race6\"]=1\n",
    "\n",
    "#ensuite on enlève toutes nos colonnes de races de notre dataset\n",
    "#Soit les colonnes: \n",
    "dataset=dataset.drop(columns=[\"applicant_race_1\",\"applicant_race_2\",\"applicant_race_3\",\"applicant_race_4\",\"applicant_race_5\"])\n",
    "#et on lui ajoute toutes nos nouvelles colonnes. \n",
    "dataset=dataset+b1+b2+b3+b4+b5+b6\"\"\"\n",
    "\n",
    "#On fait pareil pour la race du co applicant. Mais cette fois ci avec 8 catégories. \n",
    "#On ne va pas faire comme dans le dataset. On va créer 8 nouvelles features.\n",
    "#Car on a quand même plus de 150 personnes qui ont plus d'une race et puis cela est toujours bon de savoir comment faire. \n",
    "#Et à chaque fois pour chaque données on va mettre un 1 ou 0 pour savoir si la donnée a cette race ou non. \n",
    "#Et sinon bien sur on met un 1 dans notre provided. \n",
    "#On met des co et des c partout comparé à au dessus. \n",
    "n=len(dataset)\n",
    "print(n)\n",
    "#On va créer nos 6 nouvelles colonnes:\n",
    "#de longueur n et de noms: race1, race2, race3, race4, race5, race6\n",
    "c1=pd.DataFrame({\"corace1\": [0 for i in range(n)]})\n",
    "c2=pd.DataFrame({\"corace2\": [0 for i in range(n)]})\n",
    "c3=pd.DataFrame({\"corace3\": [0 for i in range(n)]})\n",
    "c4=pd.DataFrame({\"corace4\": [0 for i in range(n)]})\n",
    "c5=pd.DataFrame({\"corace5\": [0 for i in range(n)]})\n",
    "c6=pd.DataFrame({\"corace6\": [0 for i in range(n)]})\n",
    "#On va parcourir les éléments de notre dataset. de taille n. \n",
    "for k in range(n):\n",
    "    #Pour chaque élément: On va regarder leurs valeurs dans le colonnes de races\n",
    "    #On a 5 colonnes de race. \n",
    "    covalrace1=dataset.loc[k, \"co_applicant_race_1\"]\n",
    "    covalrace2=dataset.loc[k, \"co_applicant_race_2\"]\n",
    "    covalrace3=dataset.loc[k, \"co_applicant_race_3\"]\n",
    "    covalrace4=dataset.loc[k, \"co_applicant_race_4\"]\n",
    "    covalrace5=dataset.loc[k, \"co_applicant_race_5\"]\n",
    "    #et le stocker dans une liste\n",
    "    Lco=[covalrace1,covalrace2,covalrace3,covalrace4,covalrace5]\n",
    "    #On va alors parcourir cette liste et en fonction de ce qu'elle contient on va remplir \n",
    "    #ou non les 6 colonnes crées avant\n",
    "    #avec 1 si oui l'élément a cette race et on fait rien sinon sinon. \n",
    "    #pour race1: \n",
    "    if 1 in Lco:\n",
    "         c1.loc[k,\"corace1\"]=1\n",
    "    #pour race2: \n",
    "    if 2 in Lco:\n",
    "         c2.loc[k,\"corace2\"]=1\n",
    "    #pour race1: \n",
    "    if 3 in Lco:\n",
    "         c3.loc[k,\"corace3\"]=1\n",
    "    #pour race1: \n",
    "    if 4 in Lco:\n",
    "         c4.loc[k,\"corace4\"]=1\n",
    "    #pour race1: \n",
    "    if 5 in Lco:\n",
    "         c5.loc[k,\"corace5\"]=1\n",
    "    #pour race1: \n",
    "    if 6 in Lco:\n",
    "         c6.loc[k,\"corace6\"]=1\n",
    "\n",
    "#ensuite on enlève toutes nos colonnes de races de notre dataset\n",
    "#Soit les colonnes: \n",
    "dataset=dataset.drop(columns=[\"co_applicant_race_1\",\"co_applicant_race_2\",\"co_applicant_race_3\",\"co_applicant_race_4\",\"co_applicant_race_5\"])\n",
    "#et on lui ajoute toutes nos nouvelles colonnes. \n",
    "dataset=dataset+c1+c2+c3+c4+c5+c6\n",
    "\n",
    "\n",
    "#On enlève les colonnes:\n",
    "#denial_reason_1, denial_reason_2, denial_reason_3 et rate_spread\n",
    "#car beaucoup trop de données manquantes\n",
    "dataset = dataset.drop(columns=[\"denial_reason_1\", \"denial_reason_2\", \"denial_reason_3\", \"rate_spread\"])\n",
    "\n",
    "#que nous n'ayons plus que des données numerical\n",
    "#pas de problèmes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36b2863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28632 entries, 0 to 28631\n",
      "Data columns (total 37 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   action_taken                    0 non-null      float64\n",
      " 1   agency_code                     0 non-null      float64\n",
      " 2   applicant_ethnicity             0 non-null      float64\n",
      " 3   applicant_income_000s           0 non-null      float64\n",
      " 4   applicant_sex                   0 non-null      float64\n",
      " 5   application_date_indicator      0 non-null      float64\n",
      " 6   census_tract_number             0 non-null      object \n",
      " 7   co_applicant_ethnicity          0 non-null      float64\n",
      " 8   co_applicant_sex                0 non-null      float64\n",
      " 9   corace1                         0 non-null      float64\n",
      " 10  corace2                         0 non-null      float64\n",
      " 11  corace3                         0 non-null      float64\n",
      " 12  corace4                         0 non-null      float64\n",
      " 13  corace5                         0 non-null      float64\n",
      " 14  corace6                         0 non-null      float64\n",
      " 15  county_code                     0 non-null      object \n",
      " 16  hud_median_family_income        0 non-null      float64\n",
      " 17  lien_status                     0 non-null      float64\n",
      " 18  loan_amount_000s                0 non-null      float64\n",
      " 19  loan_purpose                    0 non-null      float64\n",
      " 20  loan_type                       0 non-null      float64\n",
      " 21  minority_population             0 non-null      float64\n",
      " 22  msamd                           0 non-null      object \n",
      " 23  number_of_1_to_4_family_units   0 non-null      float64\n",
      " 24  number_of_owner_occupied_units  0 non-null      float64\n",
      " 25  owner_occupancy                 0 non-null      float64\n",
      " 26  population                      0 non-null      float64\n",
      " 27  preapproval                     0 non-null      float64\n",
      " 28  property_type                   0 non-null      float64\n",
      " 29  purchaser_type                  0 non-null      float64\n",
      " 30  race1                           0 non-null      float64\n",
      " 31  race2                           0 non-null      float64\n",
      " 32  race3                           0 non-null      float64\n",
      " 33  race4                           0 non-null      float64\n",
      " 34  race5                           0 non-null      float64\n",
      " 35  race6                           0 non-null      float64\n",
      " 36  tract_to_msamd_income           0 non-null      float64\n",
      "dtypes: float64(34), object(3)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da4d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d. Création de la pipeline\n",
    "#on créer et utiliser un modèle très simple ici le but surtout était de faire du feature engineering. \n",
    "#Une simple régression linéaire peut être une bonne idée. \n",
    "#On rappel qu'ici il s'agit de créer une ia supervisée. A voir les modèles auquels cela correspond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1049946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#e. Création des datasets d’entraînement et de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0472d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f. Séparation des target des datas dans les datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e7d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#g. Entraînement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb1d0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h. Evaluation sur les données tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "488f844d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m data, target = dataset.drop(columns=target_name), dataset[target_name]\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mmodel_pipeline_full\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# predictive model\u001b[39;00m\n\u001b[32m     15\u001b[39m y_pred=model_pipeline_full.predict(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:359\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[32m    372\u001b[39m estimator = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimator)(criterion=\u001b[38;5;28mself\u001b[39m.criterion)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1385\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m   1368\u001b[39m X = check_array(\n\u001b[32m   1369\u001b[39m     X,\n\u001b[32m   1370\u001b[39m     accept_sparse=accept_sparse,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1382\u001b[39m     input_name=\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1383\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m y = \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n\u001b[32m   1389\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1395\u001b[39m, in \u001b[36m_check_y\u001b[39m\u001b[34m(y, multi_output, y_numeric, estimator)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m     y = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1405\u001b[39m     estimator_name = _check_estimator_name(estimator)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "# model pipeloine and scaling complete\n",
    "model_pipeline_full=Pipeline([ \n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# data preparation\n",
    "target_name = \"action_taken\"\n",
    "data, target = dataset.drop(columns=target_name), dataset[target_name]\n",
    "\n",
    "\n",
    "# training\n",
    "model_pipeline_full.fit(data,target)\n",
    "\n",
    "# predictive model\n",
    "y_pred=model_pipeline_full.predict(data)\n",
    "\n",
    "# evaluation of the model using cross validation\n",
    "\n",
    "cv_results= cross_validate(\n",
    "    model_pipeline_full,\n",
    "    data,\n",
    "    target,\n",
    "    cv=StratifiedKFold(n_splits=10),\n",
    "    scoring= 'accuracy',\n",
    "    return_train_score= True,\n",
    "    return_estimator= True\n",
    ")\n",
    "# print the accuracy\n",
    "\n",
    "mean_accuracy_full = cv_results['test_score'].mean()\n",
    "print(f\"Accuracy_full  (10-fold CV) : {mean_accuracy_full:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
