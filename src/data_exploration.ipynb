{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80fb375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa0d272f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15932\\1282787046.py:2: DtypeWarning: Columns (36,38,46,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset=pd.read_csv(\"https://raw.githubusercontent.com/Jomuel-net/AI-finance-project/refs/heads/main/Data%20set/data_bank_alaska.csv\")\n"
     ]
    }
   ],
   "source": [
    "#importation du dataset\n",
    "dataset=pd.read_csv(\"https://raw.githubusercontent.com/Jomuel-net/AI-finance-project/refs/heads/main/Data%20set/data_bank_alaska.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4be19f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notre objectif va être de se baser sur ce dataset et de créer un modèle.\n",
    "#On va faire un module par partie nécessaire de notre modèle. et on save indépendamment du github de notre côté afin\n",
    "#d'être sur à la fin de notre travail aujourd'hui. On télécharge data_exploration à l'aide de file puis Save AS..\n",
    "#on va coder tout notre modèle ici afin de simplifier pour l'instant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "35484757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28632 entries, 0 to 28631\n",
      "Data columns (total 78 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   as_of_year                      28632 non-null  int64  \n",
      " 1   respondent_id                   28632 non-null  object \n",
      " 2   agency_name                     28632 non-null  object \n",
      " 3   agency_abbr                     28632 non-null  object \n",
      " 4   agency_code                     28632 non-null  int64  \n",
      " 5   loan_type_name                  28632 non-null  object \n",
      " 6   loan_type                       28632 non-null  int64  \n",
      " 7   property_type_name              28632 non-null  object \n",
      " 8   property_type                   28632 non-null  int64  \n",
      " 9   loan_purpose_name               28632 non-null  object \n",
      " 10  loan_purpose                    28632 non-null  int64  \n",
      " 11  owner_occupancy_name            28632 non-null  object \n",
      " 12  owner_occupancy                 28632 non-null  int64  \n",
      " 13  loan_amount_000s                28632 non-null  int64  \n",
      " 14  preapproval_name                28632 non-null  object \n",
      " 15  preapproval                     28632 non-null  int64  \n",
      " 16  action_taken_name               28632 non-null  object \n",
      " 17  action_taken                    28632 non-null  int64  \n",
      " 18  msamd_name                      22213 non-null  object \n",
      " 19  msamd                           22213 non-null  float64\n",
      " 20  state_name                      28632 non-null  object \n",
      " 21  state_abbr                      28632 non-null  object \n",
      " 22  state_code                      28632 non-null  int64  \n",
      " 23  county_name                     28514 non-null  object \n",
      " 24  county_code                     28523 non-null  float64\n",
      " 25  census_tract_number             28521 non-null  float64\n",
      " 26  applicant_ethnicity_name        28632 non-null  object \n",
      " 27  applicant_ethnicity             28632 non-null  int64  \n",
      " 28  co_applicant_ethnicity_name     28632 non-null  object \n",
      " 29  co_applicant_ethnicity          28632 non-null  int64  \n",
      " 30  applicant_race_name_1           28632 non-null  object \n",
      " 31  applicant_race_1                28632 non-null  int64  \n",
      " 32  applicant_race_name_2           551 non-null    object \n",
      " 33  applicant_race_2                551 non-null    float64\n",
      " 34  applicant_race_name_3           49 non-null     object \n",
      " 35  applicant_race_3                49 non-null     float64\n",
      " 36  applicant_race_name_4           24 non-null     object \n",
      " 37  applicant_race_4                24 non-null     float64\n",
      " 38  applicant_race_name_5           20 non-null     object \n",
      " 39  applicant_race_5                20 non-null     float64\n",
      " 40  co_applicant_race_name_1        28632 non-null  object \n",
      " 41  co_applicant_race_1             28632 non-null  int64  \n",
      " 42  co_applicant_race_name_2        169 non-null    object \n",
      " 43  co_applicant_race_2             169 non-null    float64\n",
      " 44  co_applicant_race_name_3        17 non-null     object \n",
      " 45  co_applicant_race_3             17 non-null     float64\n",
      " 46  co_applicant_race_name_4        6 non-null      object \n",
      " 47  co_applicant_race_4             6 non-null      float64\n",
      " 48  co_applicant_race_name_5        5 non-null      object \n",
      " 49  co_applicant_race_5             5 non-null      float64\n",
      " 50  applicant_sex_name              28632 non-null  object \n",
      " 51  applicant_sex                   28632 non-null  int64  \n",
      " 52  co_applicant_sex_name           28632 non-null  object \n",
      " 53  co_applicant_sex                28632 non-null  int64  \n",
      " 54  applicant_income_000s           25327 non-null  float64\n",
      " 55  purchaser_type_name             28632 non-null  object \n",
      " 56  purchaser_type                  28632 non-null  int64  \n",
      " 57  denial_reason_name_1            2209 non-null   object \n",
      " 58  denial_reason_1                 2209 non-null   float64\n",
      " 59  denial_reason_name_2            503 non-null    object \n",
      " 60  denial_reason_2                 503 non-null    float64\n",
      " 61  denial_reason_name_3            50 non-null     object \n",
      " 62  denial_reason_3                 50 non-null     float64\n",
      " 63  rate_spread                     254 non-null    float64\n",
      " 64  hoepa_status_name               28632 non-null  object \n",
      " 65  hoepa_status                    28632 non-null  int64  \n",
      " 66  lien_status_name                28632 non-null  object \n",
      " 67  lien_status                     28632 non-null  int64  \n",
      " 68  edit_status_name                0 non-null      float64\n",
      " 69  edit_status                     0 non-null      float64\n",
      " 70  sequence_number                 0 non-null      float64\n",
      " 71  population                      28521 non-null  float64\n",
      " 72  minority_population             28521 non-null  float64\n",
      " 73  hud_median_family_income        28521 non-null  float64\n",
      " 74  tract_to_msamd_income           28521 non-null  float64\n",
      " 75  number_of_owner_occupied_units  28521 non-null  float64\n",
      " 76  number_of_1_to_4_family_units   28521 non-null  float64\n",
      " 77  application_date_indicator      0 non-null      float64\n",
      "dtypes: float64(26), int64(19), object(33)\n",
      "memory usage: 17.0+ MB\n"
     ]
    }
   ],
   "source": [
    "#a. Récupération des datasets\n",
    "#on a récupérer au dessus notre dataset\n",
    "#test de bonne récupération. Et que l'on pourra facilement le gérer par la suite:\n",
    "#un brin de feature engineering pour cela:\n",
    "dataset.info()\n",
    "#On a bien notre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "717d34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. Importations \n",
    "#Que l'on remplira au fur et à mesure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b05b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15932\\1631887854.py:50: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dataset[[\"msamd\", \"county_code\", \"census_tract_number\"]] = dataset[[\"msamd\", \"county_code\", \"census_tract_number\"]].applymap(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28632\n"
     ]
    }
   ],
   "source": [
    "#c. Feature Engineering\n",
    "#Rien d'implémenter de connu pour faire ce que l'on doit faire. \n",
    "#Le gros du travail qui va être fait: \n",
    "#On va partir du principe faut que l'on s'assure:\n",
    "#les colonnes dont nous devons nous occuper:\n",
    "\n",
    "#les colonnes à supprimer: on va en récupérer les noms \n",
    "#on va récupérer les inutiles et celles qui forment des doublons. Par exemple on a des colonnes dont on a du data \n",
    "#categorical et ensuite sa transcription en numerical. On garde alors juste la transcription en numerical\n",
    "#Les colonnes que l'on enlève et l'explication qui va avec: \n",
    "# Les inutiles\n",
    "#la première: as_of_year, respondent_id,  state_abbr, state_code, hoepa_status, edit_status, sequence_number,\n",
    "#application_date_indicator\n",
    "dataset= dataset.drop(columns=[\"as_of_year\", \"respondent_id\",  \"state_abbr\", \"state_code\", \"hoepa_status\", \"edit_status\", \"sequence_number\"])\n",
    "\n",
    "#Toutes celles où l'on a la transposition en numerical ensuite:\n",
    "#agency_name, agency_abbr, loan_type_name, property_ype_name, loan_purpose_name, owner_occupancy_name,\n",
    "#preapproval_name, action_taken_name, msamd_name, county_name, applicant_ethnicity_name, co_applicant_ethnicity_name,\n",
    "#applicant_race_name_1, applicant_race_name_2, applicant_race_name_3, applicant_race_name_4, applicant_race_name_5\n",
    "#co_applicant_race_name_1, co_applicant_race_name_2, co_applicant_race_name_3, co_applicant_race_name_4,\n",
    "#co_applicant_race_name_5, applicant_sex_name, co_applicant_sex_name, purchaser_type_name, \n",
    "#denial_reason_name_1, denial_reason_name_2, denial_reason_name_3, hoepa_status_name, lien_status_name\n",
    "#edit_status_name, state_name,\n",
    "#toutes les colonnes où il y a name en fait. \n",
    "dataset= dataset.drop(columns=[\"agency_name\", \"agency_abbr\", \"loan_type_name\", \"property_type_name\", \"loan_purpose_name\", \"owner_occupancy_name\",\n",
    "    \"preapproval_name\", \"action_taken_name\", \"msamd_name\", \"county_name\", \"applicant_ethnicity_name\", \"co_applicant_ethnicity_name\",\n",
    "    \"applicant_race_name_1\", \"applicant_race_name_2\", \"applicant_race_name_3\", \"applicant_race_name_4\", \"applicant_race_name_5\",\n",
    "    \"co_applicant_race_name_1\", \"co_applicant_race_name_2\", \"co_applicant_race_name_3\", \"co_applicant_race_name_4\",\n",
    "    \"co_applicant_race_name_5\", \"applicant_sex_name\", \"co_applicant_sex_name\", \"purchaser_type_name\", \n",
    "    \"denial_reason_name_1\", \"denial_reason_name_2\", \"denial_reason_name_3\", \"hoepa_status_name\", \"lien_status_name\",\n",
    "    \"edit_status_name\", \"state_name\"])\n",
    "\n",
    "\n",
    "#que toutes les colonnes soient remplies. Quitte à en supprimer certaines ou en créer de nouvelles.\n",
    "\n",
    "#A mettre des 1 dans les vides cases vides pour:    \n",
    "# msamd, county_code, census_tract_number\n",
    "#Pourquoi : \n",
    "#pour msamd les colonnes non remplis c'est que le logement ne se trouve pas dans une métropole de l'Alaska\n",
    "#On peut alors convenir d'un nombre pour ce fait de ne pas être dans une métropole. \n",
    "#pour les county de même on a tout une partie de l'alaska qui n'appartient à aucun coutny et est dirigé directement\n",
    "#par l'état. \n",
    "#Attention toujours deux valeurs vides dans notre tableau sur excel. A faire gaffe si pareil sur csv. \n",
    "#On va créer la fonctio qui va modifier nos colonnes. Valeur par valeur\n",
    "def f(a):\n",
    "    if a==None: \n",
    "            #on met 1 à place\n",
    "            a=1\n",
    "#et ensuite modifier chaque colonne avec \n",
    "dataset[[\"msamd\", \"county_code\", \"census_tract_number\"]] = dataset[[\"msamd\", \"county_code\", \"census_tract_number\"]].applymap(f)\n",
    "\n",
    "#pour la race de l'applicant. On ne va pas faire comme dans le dataset. On va créer 6 nouvelles features.\n",
    "#Car on a quand même plus de 500 personnes qui ont plus d'une race et puis cela est toujours bon de savoir comment faire. \n",
    "#Et à chaque fois pour chaque données on va mettre un 1 ou 0 pour savoir si la donnée a cette race ou non. \n",
    "#Et sinon bien sur on met un 1 dans notre provided. \n",
    "n=len(dataset)\n",
    "print(n)\n",
    "#On va créer nos 6 nouvelles colonnes:\n",
    "#de longueur n et de noms: race1, race2, race3, race4, race5, race6\n",
    "b1=pd.DataFrame({\"race1\": [0 for i in range(n)]})\n",
    "b2=pd.DataFrame({\"race2\": [0 for i in range(n)]})\n",
    "b3=pd.DataFrame({\"race3\": [0 for i in range(n)]})\n",
    "b4=pd.DataFrame({\"race4\": [0 for i in range(n)]})\n",
    "b5=pd.DataFrame({\"race5\": [0 for i in range(n)]})\n",
    "b6=pd.DataFrame({\"race6\": [0 for i in range(n)]})\n",
    "#On va parcourir les éléments de notre dataset. de taille n. \n",
    "for k in range(n):\n",
    "    #Pour chaque élément: On va regarder leurs valeurs dans le colonnes de races\n",
    "    #On a 5 colonnes de race. \n",
    "    valrace1=dataset.loc[k, \"applicant_race_1\"]\n",
    "    valrace2=dataset.loc[k, \"applicant_race_2\"]\n",
    "    valrace3=dataset.loc[k, \"applicant_race_3\"]\n",
    "    valrace4=dataset.loc[k, \"applicant_race_4\"]\n",
    "    valrace5=dataset.loc[k, \"applicant_race_5\"]\n",
    "    #et le stocker dans une liste\n",
    "    L=[valrace1,valrace2,valrace3,valrace4,valrace5]\n",
    "    #On va alors parcourir cette liste et en fonction de ce qu'elle contient on va remplir \n",
    "    #ou non les 6 colonnes crées avant\n",
    "    #avec 1 si oui l'élément a cette race et on fait rien sinon sinon. \n",
    "    #pour race1: \n",
    "    if 1 in L:\n",
    "         b1.loc[k,\"race1\"]=1\n",
    "    #pour race2: \n",
    "    if 2 in L:\n",
    "         b2.loc[k,\"race2\"]=1\n",
    "    #pour race1: \n",
    "    if 3 in L:\n",
    "         b3.loc[k,\"race3\"]=1\n",
    "    #pour race1: \n",
    "    if 4 in L:\n",
    "         b4.loc[k,\"race4\"]=1\n",
    "    #pour race1: \n",
    "    if 5 in L:\n",
    "         b5.loc[k,\"race5\"]=1\n",
    "    #pour race1: \n",
    "    if 6 in L:\n",
    "         b6.loc[k,\"race6\"]=1\n",
    "\n",
    "#ensuite on enlève toutes nos colonnes de races de notre dataset\n",
    "#Soit les colonnes: \n",
    "dataset.drop(columns=[\"applicant_race_1\",\"applicant_race_2\",\"applicant_race_3\",\"applicant_race_4\",\"applicant_race_5\"])\n",
    "#et on lui ajoute toutes nos nouvelles colonnes. \n",
    "dataset=dataset+b1+b2+b3+b4+b5+b6\n",
    "\n",
    "#On fait pareil pour la race du co applicant. Mais cette fois ci avec 8 catégories. \n",
    "#On ne va pas faire comme dans le dataset. On va créer 8 nouvelles features.\n",
    "#Car on a quand même plus de 150 personnes qui ont plus d'une race et puis cela est toujours bon de savoir comment faire. \n",
    "#Et à chaque fois pour chaque données on va mettre un 1 ou 0 pour savoir si la donnée a cette race ou non. \n",
    "#Et sinon bien sur on met un 1 dans notre provided. \n",
    "#On met des co et des c partout comparé à au dessus. \n",
    "n=len(dataset)\n",
    "print(n)\n",
    "#On va créer nos 6 nouvelles colonnes:\n",
    "#de longueur n et de noms: race1, race2, race3, race4, race5, race6\n",
    "c1=pd.DataFrame({\"corace1\": [0 for i in range(n)]})\n",
    "c2=pd.DataFrame({\"corace2\": [0 for i in range(n)]})\n",
    "c3=pd.DataFrame({\"corace3\": [0 for i in range(n)]})\n",
    "c4=pd.DataFrame({\"corace4\": [0 for i in range(n)]})\n",
    "c5=pd.DataFrame({\"corace5\": [0 for i in range(n)]})\n",
    "c6=pd.DataFrame({\"corace6\": [0 for i in range(n)]})\n",
    "#On va parcourir les éléments de notre dataset. de taille n. \n",
    "for k in range(n):\n",
    "    #Pour chaque élément: On va regarder leurs valeurs dans le colonnes de races\n",
    "    #On a 5 colonnes de race. \n",
    "    covalrace1=dataset.loc[k, \"co_applicant_race_1\"]\n",
    "    covalrace2=dataset.loc[k, \"co_applicant_race_2\"]\n",
    "    covalrace3=dataset.loc[k, \"co_applicant_race_3\"]\n",
    "    covalrace4=dataset.loc[k, \"co_applicant_race_4\"]\n",
    "    covalrace5=dataset.loc[k, \"co_applicant_race_5\"]\n",
    "    #et le stocker dans une liste\n",
    "    Lco=[covalrace1,covalrace2,covalrace3,covalrace4,covalrace5]\n",
    "    #On va alors parcourir cette liste et en fonction de ce qu'elle contient on va remplir \n",
    "    #ou non les 6 colonnes crées avant\n",
    "    #avec 1 si oui l'élément a cette race et on fait rien sinon sinon. \n",
    "    #pour race1: \n",
    "    if 1 in Lco:\n",
    "         c1.loc[k,\"corace1\"]=1\n",
    "    #pour race2: \n",
    "    if 2 in Lco:\n",
    "         c2.loc[k,\"corace2\"]=1\n",
    "    #pour race1: \n",
    "    if 3 in Lco:\n",
    "         c3.loc[k,\"corace3\"]=1\n",
    "    #pour race1: \n",
    "    if 4 in Lco:\n",
    "         c4.loc[k,\"corace4\"]=1\n",
    "    #pour race1: \n",
    "    if 5 in Lco:\n",
    "         c5.loc[k,\"corace5\"]=1\n",
    "    #pour race1: \n",
    "    if 6 in Lco:\n",
    "         c6.loc[k,\"corace6\"]=1\n",
    "\n",
    "#ensuite on enlève toutes nos colonnes de races de notre dataset\n",
    "#Soit les colonnes: \n",
    "dataset.drop(columns=[\"co_applicant_race_1\",\"co_applicant_race_2\",\"co_applicant_race_3\",\"co_applicant_race_4\",\"co_applicant_race_5\"])\n",
    "#et on lui ajoute toutes nos nouvelles colonnes. \n",
    "dataset=dataset+c1+c2+c3+c4+c5+c6\n",
    "\n",
    "\n",
    "#On doit aussi supprimer :\n",
    "#purchaser_type_name\n",
    "#Car ce n'est pas donné quand le demandeur n'a pas obtenu son prêt. \n",
    "\n",
    "#On enlève les colonnes:\n",
    "#denial_reason_1, denial_reason_2, denial_reason_3 et rate_spread\n",
    "#car beaucoup trop de données manquantes\n",
    "\n",
    "#Eléments à supprimer: \n",
    "#On va enlever les éléments qui ont des cases  vides dans: \n",
    "#C'est que ce sont des logements saisonniers/temporaires dans une zone de l'alaska sous sous peuplé.\n",
    "#Que 75000 habitants sur plusieurs millions. \n",
    "#Sinon on perd des données c'est dommage et si on les remplace par des 0 cela risque de mal influencer notre modèle. \n",
    "#ceux qui n'ont pas de county_code \n",
    "#et pour le:\n",
    "#applicant_income_000s\n",
    "#On doit supprimer les éléments qui ont vide pour cette valeur. \n",
    "#Peut être le mettre plus haut pour simplifier les boucles for précédentes. \n",
    "\n",
    "\n",
    "#que nous n'ayons plus que des données numerical\n",
    "#pas de problèmes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6da4d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d. Création de la pipeline\n",
    "#on créer et utiliser un modèle très simple ici le but surtout était de faire du feature engineering. \n",
    "#Une simple régression linéaire peut être une bonne idée. \n",
    "#On rappel qu'ici il s'agit de créer une ia supervisée. A voir les modèles auquels cela correspond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1049946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#e. Création des datasets d’entraînement et de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0472d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f. Séparation des target des datas dans les datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b2e7d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#g. Entraînement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fb1d0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h. Evaluation sur les données tests\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
